{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwzs0N0g6J6l"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nnuk0dys42b0",
    "outputId": "5f564368-975e-4040-d868-3834d7bdb68d"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive\n",
    "from numpy import cumsum\n",
    "from os import walk\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, DepthwiseConv2D\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend as K\n",
    "\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BK3PvS1Q1Xom"
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def abundant(df):    \n",
    "    second = 3\n",
    "\n",
    "    l = df['second']\n",
    "    l = np.array(l)\n",
    "    l_check = np.append(l[1:],l[-1])\n",
    "    l_diff = l_check - l\n",
    "    l_ind = l_diff <= second  \n",
    "    df = df[l_ind]\n",
    "\n",
    "    return df\n",
    "\n",
    "def min_max(lst,mn,mx):\n",
    "    lst = np.array(lst)\n",
    "    lst[lst < mn] = mn\n",
    "    lst[lst > mx] = mx\n",
    "    return lst  \n",
    "\n",
    "def preprocessing(data,maximum_accuracy,low_speed):\n",
    "  \n",
    "    # Clean high accuracy and low_speed\n",
    "    data = data[data['Accuracy'] <= maximum_accuracy]\n",
    "    data = data[data['Speed'] >= low_speed]\n",
    "\n",
    "    # Clean abundant data\n",
    "    data = data.groupby('bookingID').apply(abundant)     \n",
    "\n",
    "    # Clean outliner\n",
    "    data['acceleration_x'] = min_max(data['acceleration_x'],-3,3) / 3\n",
    "    data['acceleration_y'] = min_max(data['acceleration_y'],-10,10) / 10\n",
    "    data['acceleration_z'] = min_max(data['acceleration_z'],-5,5) / 5\n",
    "    data['gyro_x'] = min_max(data['gyro_x'],-5,5) / 5\n",
    "    data['gyro_y'] = min_max(data['gyro_x'],-0.4,0.4) / 0.4\n",
    "    data['gyro_z'] = min_max(data['gyro_x'],-0.3,0.3) / 0.3 \n",
    "    data['Speed'] = min_max(data['Speed'],0,100) / 100 \n",
    "\n",
    "    # Remove columns\n",
    "    data = data[['bookingID','acceleration_x', 'acceleration_y','acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z','Speed']]  \n",
    "\n",
    "    # Total acc , gyro\n",
    "    data['total_acceleration'] = np.sqrt(np.power(data['acceleration_x'], 2) + np.power(data['acceleration_y'], 2) + np.power(data['acceleration_z'], 2))\n",
    "    data['total_gyro'] = np.sqrt(np.power(data['gyro_x'], 2) + np.power(data['gyro_y'], 2) + np.power(data['gyro_z'], 2))  \n",
    "\n",
    "    #Change format \n",
    "    col = data.columns\n",
    "    data = pd.DataFrame(data.values,columns= col)\n",
    "\n",
    "    return data \n",
    "\n",
    "window_size = 120\n",
    "def slice_(df,w = window_size):\n",
    "    nrow = df.shape[0]\n",
    "    lenght = math.floor(nrow/w) * w\n",
    "    df = df.head(lenght) \n",
    "    return df\n",
    "\n",
    "def frame(df,window_size):\n",
    "    col = df.columns\n",
    "    df = df.groupby('bookingID').apply(slice_)\n",
    "    df = pd.DataFrame(df.values,columns= col)\n",
    "    df = df[['acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','Speed','total_acceleration','total_gyro']]\n",
    "\n",
    "    n = int(df.shape[0] / window_size)\n",
    "    df = df.values.reshape(n,window_size,3,3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0_dpIOST0co"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "-kK9oMDGzOHp",
    "outputId": "16be299f-dad4-48ce-b3bb-1242613b5c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n"
     ]
    }
   ],
   "source": [
    "mypath = '/content/gdrive/My Drive/Grab/features/'\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):    \n",
    "    for file in filenames:\n",
    "        if not file.startswith('.'):\n",
    "            print(file)\n",
    "            f = mypath + file\n",
    "            df = pd.read_csv(f)\n",
    "            dataset = pd.concat([dataset,df])\n",
    "\n",
    "labels = pd.read_csv(\"/content/gdrive/My Drive/Grab/labels.csv\")\n",
    "dataset = dataset.sort_values(by=['bookingID','second'])        \n",
    "dataset['Speed'] = dataset['Speed'] * 18/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaFSV62D2DZW"
   },
   "outputs": [],
   "source": [
    "df_cln = preprocessing(dataset,maximum_accuracy = 30, low_speed= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hXta4ZZwnwf"
   },
   "outputs": [],
   "source": [
    "window_size = 120\n",
    "s = df_cln.groupby('bookingID').size()\n",
    "s = s.reset_index()\n",
    "s.columns = ['bookingID','count']\n",
    "s = s.loc[s['count']>=window_size]\n",
    "s = list(s['bookingID'])\n",
    "\n",
    "df_cln = df_cln.merge(labels,on = 'bookingID')\n",
    "df_cln = df_cln[df_cln['bookingID'].isin(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Opnd_vWaFfvk"
   },
   "source": [
    "#### Splitting Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GQrMH3ViJoqE",
    "outputId": "34e69a39-a52f-41dc-b6d9-496567543658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels 1 : (23226, 120, 3, 3)\n",
      "labels 0 : (54634, 120, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "df_cln_1 = df_cln[df_cln['label'] == 1]\n",
    "df_cln_0 = df_cln[df_cln['label'] == 0]\n",
    "\n",
    "df_cln_1 = frame(df_cln_1,window_size)\n",
    "df_cln_0 = frame(df_cln_0,window_size)\n",
    "print(\"labels 1 :\",df_cln_1.shape)\n",
    "print(\"labels 0 :\",df_cln_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2gweVS1S2GA"
   },
   "outputs": [],
   "source": [
    "no_1 = len(df_cln_1)\n",
    "no_0 = len(df_cln_0)\n",
    "labels = pd.Series(np.append(np.repeat(1, no_1),np.repeat(0, no_0)))\n",
    "label = np.asarray(pd.get_dummies(labels), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCuBkyrVVmmf"
   },
   "outputs": [],
   "source": [
    "X = np.append(df_cln_1,df_cln_0,axis = 0)\n",
    "X.shape\n",
    "\n",
    "del df_cln_1,df_cln_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "F8ojUOTVwn6r",
    "outputId": "3f98ce45-7966-4535-d545-d17ce99b1b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (62288, 120, 3, 3)\n",
      "X_test:  (15572, 120, 3, 3)\n",
      "y_train:  (62288, 2)\n",
      "y_test:  (15572, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2 , random_state=456)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print(\"X_train: \",X_train.shape)\n",
    "print('X_test: ',X_test.shape)\n",
    "\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print('y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmYTPE8d2fs7"
   },
   "outputs": [],
   "source": [
    "window_size =120\n",
    "batch_size = 128\n",
    "input_shape = (window_size,3,3)\n",
    "output = 2\n",
    "learning_rate = 0.005\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfwxYevFGvQQ"
   },
   "source": [
    "#### Deptwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "GIqRq0st2fxO",
    "outputId": "12052988-15c4-49d0-8c71-61bee8ac85c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d_1 (Depthwis (None, 120, 3, 3)         93        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 120, 3, 3)         48        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1080)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               138368    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 138,767\n",
      "Trainable params: 138,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(DepthwiseConv2D(kernel_size = (10,3), activation = 'relu', padding = 'same',input_shape=input_shape))\n",
    "model.add(DepthwiseConv2D(kernel_size = (5,3), activation = 'relu', padding = 'same',input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(output, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPL0GyLW2fz4"
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.rmsprop(lr=learning_rate, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= opt,\n",
    "              metrics=[auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5151
    },
    "colab_type": "code",
    "id": "yNVpyKny2f4e",
    "outputId": "edd5c0d6-c1f3-4003-b747-1e694796fc44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62288 samples, validate on 15572 samples\n",
      "Epoch 1/150\n",
      "62288/62288 [==============================] - 5s 82us/step - loss: 0.6124 - auc: 0.7051 - val_loss: 0.6095 - val_auc: 0.7093\n",
      "Epoch 2/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.6058 - auc: 0.7098 - val_loss: 0.6081 - val_auc: 0.7107\n",
      "Epoch 3/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.6034 - auc: 0.7119 - val_loss: 0.6057 - val_auc: 0.7124\n",
      "Epoch 4/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.6014 - auc: 0.7136 - val_loss: 0.6079 - val_auc: 0.7144\n",
      "Epoch 5/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.5999 - auc: 0.7155 - val_loss: 0.6105 - val_auc: 0.7163\n",
      "Epoch 6/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5981 - auc: 0.7173 - val_loss: 0.6133 - val_auc: 0.7181\n",
      "Epoch 7/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5966 - auc: 0.7192 - val_loss: 0.6136 - val_auc: 0.7199\n",
      "Epoch 8/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.5945 - auc: 0.7210 - val_loss: 0.6182 - val_auc: 0.7216\n",
      "Epoch 9/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.5919 - auc: 0.7226 - val_loss: 0.6209 - val_auc: 0.7231\n",
      "Epoch 10/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5888 - auc: 0.7240 - val_loss: 0.6230 - val_auc: 0.7246\n",
      "Epoch 11/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5842 - auc: 0.7257 - val_loss: 0.6420 - val_auc: 0.7264\n",
      "Epoch 12/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5783 - auc: 0.7275 - val_loss: 0.6605 - val_auc: 0.7283\n",
      "Epoch 13/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5703 - auc: 0.7295 - val_loss: 0.6496 - val_auc: 0.7304\n",
      "Epoch 14/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5620 - auc: 0.7318 - val_loss: 0.6897 - val_auc: 0.7329\n",
      "Epoch 15/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5526 - auc: 0.7345 - val_loss: 0.6872 - val_auc: 0.7356\n",
      "Epoch 16/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5428 - auc: 0.7371 - val_loss: 0.7033 - val_auc: 0.7384\n",
      "Epoch 17/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5335 - auc: 0.7400 - val_loss: 0.7301 - val_auc: 0.7413\n",
      "Epoch 18/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.5205 - auc: 0.7430 - val_loss: 0.8023 - val_auc: 0.7443\n",
      "Epoch 19/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.5090 - auc: 0.7460 - val_loss: 0.7816 - val_auc: 0.7474\n",
      "Epoch 20/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4959 - auc: 0.7492 - val_loss: 0.8397 - val_auc: 0.7507\n",
      "Epoch 21/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.4827 - auc: 0.7525 - val_loss: 0.9099 - val_auc: 0.7540\n",
      "Epoch 22/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.4715 - auc: 0.7559 - val_loss: 0.9490 - val_auc: 0.7574\n",
      "Epoch 23/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4593 - auc: 0.7592 - val_loss: 0.9254 - val_auc: 0.7607\n",
      "Epoch 24/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.4465 - auc: 0.7626 - val_loss: 0.9174 - val_auc: 0.7642\n",
      "Epoch 25/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4353 - auc: 0.7660 - val_loss: 1.0518 - val_auc: 0.7675\n",
      "Epoch 26/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4259 - auc: 0.7692 - val_loss: 1.0236 - val_auc: 0.7707\n",
      "Epoch 27/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4155 - auc: 0.7725 - val_loss: 1.1233 - val_auc: 0.7740\n",
      "Epoch 28/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.4057 - auc: 0.7757 - val_loss: 1.1624 - val_auc: 0.7771\n",
      "Epoch 29/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3973 - auc: 0.7788 - val_loss: 1.0952 - val_auc: 0.7802\n",
      "Epoch 30/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3895 - auc: 0.7819 - val_loss: 1.1577 - val_auc: 0.7833\n",
      "Epoch 31/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3806 - auc: 0.7849 - val_loss: 1.1489 - val_auc: 0.7863\n",
      "Epoch 32/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3754 - auc: 0.7877 - val_loss: 1.3564 - val_auc: 0.7890\n",
      "Epoch 33/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3678 - auc: 0.7904 - val_loss: 1.3430 - val_auc: 0.7917\n",
      "Epoch 34/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3621 - auc: 0.7931 - val_loss: 1.2553 - val_auc: 0.7944\n",
      "Epoch 35/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3592 - auc: 0.7957 - val_loss: 1.3185 - val_auc: 0.7969\n",
      "Epoch 36/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3525 - auc: 0.7983 - val_loss: 1.4856 - val_auc: 0.7994\n",
      "Epoch 37/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3457 - auc: 0.8006 - val_loss: 1.4298 - val_auc: 0.8018\n",
      "Epoch 38/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3406 - auc: 0.8030 - val_loss: 1.4847 - val_auc: 0.8041\n",
      "Epoch 39/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3363 - auc: 0.8053 - val_loss: 1.4984 - val_auc: 0.8064\n",
      "Epoch 40/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3359 - auc: 0.8075 - val_loss: 1.6209 - val_auc: 0.8084\n",
      "Epoch 41/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3293 - auc: 0.8095 - val_loss: 1.4478 - val_auc: 0.8105\n",
      "Epoch 42/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.3256 - auc: 0.8115 - val_loss: 1.5929 - val_auc: 0.8124\n",
      "Epoch 43/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.3188 - auc: 0.8134 - val_loss: 1.6487 - val_auc: 0.8144\n",
      "Epoch 44/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3198 - auc: 0.8154 - val_loss: 1.6703 - val_auc: 0.8162\n",
      "Epoch 45/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3132 - auc: 0.8172 - val_loss: 1.6810 - val_auc: 0.8180\n",
      "Epoch 46/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3154 - auc: 0.8189 - val_loss: 1.7955 - val_auc: 0.8196\n",
      "Epoch 47/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3127 - auc: 0.8205 - val_loss: 1.7104 - val_auc: 0.8212\n",
      "Epoch 48/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3101 - auc: 0.8220 - val_loss: 1.8326 - val_auc: 0.8228\n",
      "Epoch 49/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3083 - auc: 0.8236 - val_loss: 1.7326 - val_auc: 0.8243\n",
      "Epoch 50/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.3040 - auc: 0.8251 - val_loss: 1.9620 - val_auc: 0.8257\n",
      "Epoch 51/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3020 - auc: 0.8264 - val_loss: 1.9121 - val_auc: 0.8270\n",
      "Epoch 52/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3024 - auc: 0.8277 - val_loss: 1.7456 - val_auc: 0.8284\n",
      "Epoch 53/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3021 - auc: 0.8291 - val_loss: 1.8406 - val_auc: 0.8297\n",
      "Epoch 54/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2989 - auc: 0.8304 - val_loss: 1.8816 - val_auc: 0.8309\n",
      "Epoch 55/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.3016 - auc: 0.8316 - val_loss: 1.9034 - val_auc: 0.8321\n",
      "Epoch 56/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2950 - auc: 0.8327 - val_loss: 1.8721 - val_auc: 0.8333\n",
      "Epoch 57/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2894 - auc: 0.8339 - val_loss: 1.9609 - val_auc: 0.8344\n",
      "Epoch 58/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2924 - auc: 0.8350 - val_loss: 1.9468 - val_auc: 0.8355\n",
      "Epoch 59/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2956 - auc: 0.8360 - val_loss: 1.8877 - val_auc: 0.8365\n",
      "Epoch 60/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2970 - auc: 0.8371 - val_loss: 2.0172 - val_auc: 0.8375\n",
      "Epoch 61/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2881 - auc: 0.8381 - val_loss: 2.0579 - val_auc: 0.8385\n",
      "Epoch 62/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2884 - auc: 0.8390 - val_loss: 1.9997 - val_auc: 0.8395\n",
      "Epoch 63/150\n",
      "62288/62288 [==============================] - 5s 77us/step - loss: 0.2887 - auc: 0.8400 - val_loss: 2.0356 - val_auc: 0.8404\n",
      "Epoch 64/150\n",
      "62288/62288 [==============================] - 5s 77us/step - loss: 0.2866 - auc: 0.8409 - val_loss: 1.9361 - val_auc: 0.8413\n",
      "Epoch 65/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2875 - auc: 0.8418 - val_loss: 2.1147 - val_auc: 0.8422\n",
      "Epoch 66/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2838 - auc: 0.8427 - val_loss: 2.1511 - val_auc: 0.8430\n",
      "Epoch 67/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2843 - auc: 0.8435 - val_loss: 2.0184 - val_auc: 0.8439\n",
      "Epoch 68/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2838 - auc: 0.8443 - val_loss: 2.0331 - val_auc: 0.8447\n",
      "Epoch 69/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2849 - auc: 0.8451 - val_loss: 2.0919 - val_auc: 0.8454\n",
      "Epoch 70/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2854 - auc: 0.8458 - val_loss: 1.8683 - val_auc: 0.8462\n",
      "Epoch 71/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2844 - auc: 0.8466 - val_loss: 2.1746 - val_auc: 0.8469\n",
      "Epoch 72/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2818 - auc: 0.8473 - val_loss: 2.0944 - val_auc: 0.8476\n",
      "Epoch 73/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2858 - auc: 0.8480 - val_loss: 2.2669 - val_auc: 0.8483\n",
      "Epoch 74/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2819 - auc: 0.8486 - val_loss: 2.0850 - val_auc: 0.8489\n",
      "Epoch 75/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2845 - auc: 0.8493 - val_loss: 2.1542 - val_auc: 0.8496\n",
      "Epoch 76/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2815 - auc: 0.8499 - val_loss: 2.0991 - val_auc: 0.8502\n",
      "Epoch 77/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2795 - auc: 0.8506 - val_loss: 2.1705 - val_auc: 0.8508\n",
      "Epoch 78/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2758 - auc: 0.8512 - val_loss: 2.0608 - val_auc: 0.8514\n",
      "Epoch 79/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2780 - auc: 0.8518 - val_loss: 2.1874 - val_auc: 0.8520\n",
      "Epoch 80/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2828 - auc: 0.8523 - val_loss: 2.3185 - val_auc: 0.8526\n",
      "Epoch 81/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2767 - auc: 0.8528 - val_loss: 2.2582 - val_auc: 0.8531\n",
      "Epoch 82/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2786 - auc: 0.8534 - val_loss: 2.1428 - val_auc: 0.8536\n",
      "Epoch 83/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2749 - auc: 0.8539 - val_loss: 2.1001 - val_auc: 0.8542\n",
      "Epoch 84/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2693 - auc: 0.8545 - val_loss: 2.1996 - val_auc: 0.8547\n",
      "Epoch 85/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2753 - auc: 0.8550 - val_loss: 1.9972 - val_auc: 0.8553\n",
      "Epoch 86/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2727 - auc: 0.8556 - val_loss: 2.3184 - val_auc: 0.8558\n",
      "Epoch 87/150\n",
      "62288/62288 [==============================] - 5s 76us/step - loss: 0.2702 - auc: 0.8561 - val_loss: 2.2155 - val_auc: 0.8563\n",
      "Epoch 88/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2740 - auc: 0.8565 - val_loss: 2.2920 - val_auc: 0.8567\n",
      "Epoch 89/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2707 - auc: 0.8570 - val_loss: 2.1266 - val_auc: 0.8572\n",
      "Epoch 90/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2696 - auc: 0.8575 - val_loss: 2.1185 - val_auc: 0.8577\n",
      "Epoch 91/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2698 - auc: 0.8579 - val_loss: 2.3164 - val_auc: 0.8581\n",
      "Epoch 92/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2746 - auc: 0.8584 - val_loss: 2.0068 - val_auc: 0.8586\n",
      "Epoch 93/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2713 - auc: 0.8588 - val_loss: 2.3180 - val_auc: 0.8590\n",
      "Epoch 94/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2706 - auc: 0.8593 - val_loss: 2.4255 - val_auc: 0.8594\n",
      "Epoch 95/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2692 - auc: 0.8596 - val_loss: 2.4756 - val_auc: 0.8598\n",
      "Epoch 96/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2712 - auc: 0.8600 - val_loss: 2.3111 - val_auc: 0.8602\n",
      "Epoch 97/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2676 - auc: 0.8604 - val_loss: 2.5182 - val_auc: 0.8606\n",
      "Epoch 98/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2652 - auc: 0.8608 - val_loss: 2.2748 - val_auc: 0.8609\n",
      "Epoch 99/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2630 - auc: 0.8612 - val_loss: 2.0190 - val_auc: 0.8614\n",
      "Epoch 100/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2666 - auc: 0.8616 - val_loss: 2.3541 - val_auc: 0.8617\n",
      "Epoch 101/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2702 - auc: 0.8619 - val_loss: 2.4704 - val_auc: 0.8621\n",
      "Epoch 102/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2658 - auc: 0.8623 - val_loss: 2.5004 - val_auc: 0.8624\n",
      "Epoch 103/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2646 - auc: 0.8626 - val_loss: 2.4244 - val_auc: 0.8628\n",
      "Epoch 104/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2632 - auc: 0.8629 - val_loss: 2.1409 - val_auc: 0.8631\n",
      "Epoch 105/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2610 - auc: 0.8633 - val_loss: 2.4970 - val_auc: 0.8635\n",
      "Epoch 106/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2694 - auc: 0.8636 - val_loss: 2.5618 - val_auc: 0.8638\n",
      "Epoch 107/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2621 - auc: 0.8639 - val_loss: 2.3800 - val_auc: 0.8641\n",
      "Epoch 108/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2563 - auc: 0.8643 - val_loss: 2.2567 - val_auc: 0.8644\n",
      "Epoch 109/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2598 - auc: 0.8646 - val_loss: 2.4411 - val_auc: 0.8648\n",
      "Epoch 110/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2566 - auc: 0.8649 - val_loss: 2.4817 - val_auc: 0.8651\n",
      "Epoch 111/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2527 - auc: 0.8652 - val_loss: 1.9597 - val_auc: 0.8654\n",
      "Epoch 112/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2551 - auc: 0.8656 - val_loss: 2.3576 - val_auc: 0.8658\n",
      "Epoch 113/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2628 - auc: 0.8659 - val_loss: 2.5592 - val_auc: 0.8660\n",
      "Epoch 114/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2605 - auc: 0.8662 - val_loss: 2.5990 - val_auc: 0.8663\n",
      "Epoch 115/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2530 - auc: 0.8665 - val_loss: 2.6506 - val_auc: 0.8666\n",
      "Epoch 116/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2608 - auc: 0.8667 - val_loss: 2.3673 - val_auc: 0.8669\n",
      "Epoch 117/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2573 - auc: 0.8670 - val_loss: 2.6754 - val_auc: 0.8672\n",
      "Epoch 118/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2519 - auc: 0.8673 - val_loss: 2.6222 - val_auc: 0.8674\n",
      "Epoch 119/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2526 - auc: 0.8676 - val_loss: 2.6945 - val_auc: 0.8677\n",
      "Epoch 120/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2560 - auc: 0.8678 - val_loss: 2.7170 - val_auc: 0.8679\n",
      "Epoch 121/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2508 - auc: 0.8681 - val_loss: 2.7917 - val_auc: 0.8682\n",
      "Epoch 122/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2535 - auc: 0.8683 - val_loss: 2.4455 - val_auc: 0.8684\n",
      "Epoch 123/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2574 - auc: 0.8686 - val_loss: 2.7819 - val_auc: 0.8687\n",
      "Epoch 124/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2559 - auc: 0.8688 - val_loss: 2.8931 - val_auc: 0.8689\n",
      "Epoch 125/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2509 - auc: 0.8690 - val_loss: 2.3615 - val_auc: 0.8691\n",
      "Epoch 126/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2503 - auc: 0.8693 - val_loss: 2.6154 - val_auc: 0.8694\n",
      "Epoch 127/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2540 - auc: 0.8695 - val_loss: 2.5075 - val_auc: 0.8696\n",
      "Epoch 128/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2511 - auc: 0.8698 - val_loss: 2.0889 - val_auc: 0.8699\n",
      "Epoch 129/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2575 - auc: 0.8701 - val_loss: 2.2876 - val_auc: 0.8702\n",
      "Epoch 130/150\n",
      "62288/62288 [==============================] - 5s 77us/step - loss: 0.2499 - auc: 0.8703 - val_loss: 2.0665 - val_auc: 0.8705\n",
      "Epoch 131/150\n",
      "62288/62288 [==============================] - 5s 77us/step - loss: 0.2543 - auc: 0.8706 - val_loss: 2.3418 - val_auc: 0.8707\n",
      "Epoch 132/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2474 - auc: 0.8709 - val_loss: 2.6022 - val_auc: 0.8710\n",
      "Epoch 133/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2514 - auc: 0.8711 - val_loss: 2.9769 - val_auc: 0.8712\n",
      "Epoch 134/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2509 - auc: 0.8713 - val_loss: 2.7335 - val_auc: 0.8714\n",
      "Epoch 135/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2485 - auc: 0.8715 - val_loss: 2.7936 - val_auc: 0.8716\n",
      "Epoch 136/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2457 - auc: 0.8717 - val_loss: 2.6688 - val_auc: 0.8718\n",
      "Epoch 137/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2495 - auc: 0.8719 - val_loss: 2.3471 - val_auc: 0.8720\n",
      "Epoch 138/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2450 - auc: 0.8722 - val_loss: 2.6359 - val_auc: 0.8723\n",
      "Epoch 139/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2476 - auc: 0.8724 - val_loss: 2.6829 - val_auc: 0.8725\n",
      "Epoch 140/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2482 - auc: 0.8726 - val_loss: 2.7737 - val_auc: 0.8727\n",
      "Epoch 141/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2487 - auc: 0.8728 - val_loss: 2.7284 - val_auc: 0.8729\n",
      "Epoch 142/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2511 - auc: 0.8730 - val_loss: 2.3325 - val_auc: 0.8731\n",
      "Epoch 143/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2489 - auc: 0.8732 - val_loss: 2.8903 - val_auc: 0.8733\n",
      "Epoch 144/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2445 - auc: 0.8734 - val_loss: 2.5382 - val_auc: 0.8735\n",
      "Epoch 145/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2432 - auc: 0.8736 - val_loss: 2.8403 - val_auc: 0.8736\n",
      "Epoch 146/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2409 - auc: 0.8737 - val_loss: 2.8591 - val_auc: 0.8738\n",
      "Epoch 147/150\n",
      "62288/62288 [==============================] - 5s 75us/step - loss: 0.2519 - auc: 0.8739 - val_loss: 2.6624 - val_auc: 0.8740\n",
      "Epoch 148/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2418 - auc: 0.8741 - val_loss: 3.2284 - val_auc: 0.8742\n",
      "Epoch 149/150\n",
      "62288/62288 [==============================] - 5s 74us/step - loss: 0.2409 - auc: 0.8743 - val_loss: 2.4923 - val_auc: 0.8744\n",
      "Epoch 150/150\n",
      "62288/62288 [==============================] - 5s 73us/step - loss: 0.2402 - auc: 0.8745 - val_loss: 2.7496 - val_auc: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb05d010198>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,batch_size = batch_size, epochs=epochs,validation_data=(X_test, y_test),shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JfR1bBBt2f9Q",
    "outputId": "e29e6e8a-e7c5-4056-dc51-c60b9c9d3784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15572/15572 [==============================] - 1s 67us/step\n",
      "Test loss: 2.7495952866360978\n",
      "Test AUC: 0.8741542684398027\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test AUC:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnJhGZV9Upqt"
   },
   "outputs": [],
   "source": [
    "model.save('/content/gdrive/My Drive/Grab/my_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAIX7TvQHPYj"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd9cTbrkZ-am"
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testset = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "LKVHvC_jpvj6",
    "outputId": "176ad64d-b40d-4486-bd1a-d8b2a0d3c1a2"
   },
   "outputs": [],
   "source": [
    "model = load_model('my_model3.h5', custom_objects={'auc': auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JD7u1fiPc4J3"
   },
   "outputs": [],
   "source": [
    "def pred_test(dataset,window_size = 120,thres = 0.5):\n",
    "    \n",
    "    id_f = np.unique(dataset['bookingID'])\n",
    "    dataset = preprocessing(dataset,30,10) \n",
    "\n",
    "    d = dataset.groupby('bookingID').size()\n",
    "    d = d.reset_index()\n",
    "    d.columns = ['bookingID','count']\n",
    "    s = list(d['bookingID'][d['count'] >= window_size])\n",
    "\n",
    "    id_lenght = d[d['count'] >= window_size]\n",
    "    id_lenght['n_frame'] = np.floor(d['count']/ window_size)\n",
    "\n",
    "    df = dataset[dataset['bookingID'].isin(s)]\n",
    "    X = frame(df,window_size)\n",
    "    m_pred = model.predict_classes(X)\n",
    "\n",
    "    ID = np.array(id_lenght['bookingID']).astype(int)\n",
    "    N = np.array(id_lenght['n_frame']).astype(int)\n",
    "    ID_N = np.array([])\n",
    "    for i,j in zip(ID,N):\n",
    "        ID_N = np.append(ID_N,np.repeat(i,j))\n",
    "\n",
    "    pred = pd.DataFrame({'bookingID' :ID_N,'pred' : m_pred}) \n",
    "    pred = pred.groupby('bookingID')['pred'].mean()\n",
    "    pred = pd.DataFrame(pred)\n",
    "    pred = pred.reset_index()\n",
    "    pred.loc[pred['pred']  > thres,'pred'] = 1\n",
    "    pred.loc[pred['pred'] <= thres,'pred'] = 0\n",
    "\n",
    "    id_f = pd.DataFrame({'bookingID':id_f})\n",
    "    id_f = id_f.merge(pred,how ='left', on = 'bookingID')\n",
    "    id_f = id_f.fillna(0)\n",
    "    return(id_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "7q96b1t-rzpI",
    "outputId": "97deb5b3-2833-424a-a8d4-ec9bfb7475b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "id_f = pred_test(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVlsWmTCtWiR"
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CongQgaBGmGa",
    "outputId": "4bb010ba-a512-440e-a7df-fed88f55f8bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9170124738875937"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_f = id_f.merge(labels,on = 'bookingID')\n",
    "roc_auc_score(id_f['pred'],id_f['label'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ten.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
